#!/usr/bin/env python3

# bamdam by Bianca De Sanctis, bddesanctis@gmail.com 

import sys 
import re
import csv
import pysam
import math
import argparse
import os
import hyperloglog
import subprocess
try: # optional library only needed for plotting 
    import matplotlib.pyplot as plt
    matplotlib_imported = True
except:
    matplotlib_imported = False
try: # optional library for progress bars 
    from tqdm import tqdm
    tqdm_imported = True
except ImportError:
    tqdm_imported = False

def get_sorting_order(file_path):
    # this is the fastest way to check if HD is the first line of the header
    # a bam is basically a gzipped sam; take advantage of this so we don't have to read in the full header just for this check (pysam can't stream it)
    command = f"gunzip -dc {file_path}"
    process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE)
    # read the output in binary and manually decode it 
    line = process.stdout.readline()
    hd_index = line.find(b'@HD')
    if hd_index != -1:
        line = line[hd_index:]
        line = line.decode('ascii')
        fields = line.strip().split("\t")
        for field in fields:
            if field.startswith("SO:"):
                sorting_order = field.split(":")[1]
                process.stdout.close()
                process.terminate()
                return sorting_order
    process.stdout.close()
    process.terminate()
    # here is a slower version, try this if HD line is not at the top
    try:
        with pysam.AlignmentFile(file_path, "rb") as bamfile:
            hdline = bamfile.header.get('HD', {})
            sorting_order = hdline.get('SO', 'unknown')
            return sorting_order
    except:
        return "unknown"

def write_shortened_lca(original_lca_path,short_lca_path,upto,mincount,exclude_keywords, exclude_under):

    print("\nWriting a filtered lca file...")

    lcaheaderlines = 0
    with open(original_lca_path, 'r') as lcafile:
        for lcaline in lcafile:
            if "root" in lcaline:
                break
            lcaheaderlines += 1
    total_short_lca_lines = 0

    # pass 1: make a dictionary with all the tax ids and their counts 
    number_counts = {}
    with open(original_lca_path, 'r') as file:
        for _ in range(lcaheaderlines):
            next(file) 
        for line in file:
            if upto in line:
                if exclude_under and any(keyword in line for keyword in exclude_keywords): # checks all nodes in the path 
                    continue
                    # if you want to exclude all nodes underneath the ones you specified too, all you have to do is look for all the keywords in every line 
                else:
                    entry = line.strip().split('\t')
                    if len(entry) > 1:  
                        fields = entry[1:] # can ditch the read id 
                        if (not exclude_under) and any(keyword in fields[0] for keyword in exclude_keywords):
                            continue # this only checks the node the read is actually assigned to 
                        else:
                            keepgoing = True; field = 0
                            while keepgoing:
                                taxid = fields[field].split(':')[0]
                                if taxid in number_counts:
                                    number_counts[taxid] += 1
                                else:
                                    number_counts[taxid] = 1
                                if upto in fields[field]:
                                    keepgoing = False
                                field +=1 

    goodnodes = [key for key, count in number_counts.items() if count >= mincount]
    # these are the nodes that have at least the min count of reads assigned to them (or below them), and which are at most upto

    # pass 2: rewrite lines into a new lca file that pass the filter
    oldreadname = ""
    with open(original_lca_path, 'r') as infile, open(short_lca_path, 'w') as outfile:
            for _ in range(lcaheaderlines):
                next(infile) # assumes it has the usual 2 comment lines at the top from ngslca
            for line in infile:
                tab_split = line.find('\t')
                newreadname = line[:tab_split].rsplit(':', 3)[0]
                if newreadname == oldreadname:
                    print("Error: You have duplicate entries in your LCA file, for example " + newreadname + ". You should fix this and re-run BamDam. Here is a suggested fix: awk '!seen[$0]++' input_lca > output_lca")
                    exit(-1)
                if upto in line:
                    # you can just go straight to the upto level and check if that node has high enough count 
                    if exclude_under and any(keyword in line for keyword in exclude_keywords):
                        continue # don't keep the ones that have the keywords anywhere if exclude_under is true
                    elif (not exclude_under) and any(keyword in entry[1] for keyword in exclude_keywords):
                        continue # don't keep the ones that have the keywords in their assignment if exclude_under is false
                    else:
                        entry = line.strip().split('\t')
                        for field in entry[1:]:
                            if f":{upto}" in field:
                                number = field.split(':')[0]
                                if number in goodnodes:
                                    outfile.write(line)
                                    total_short_lca_lines += 1
                                    break

    print("Wrote a filtered lca file. \n")

    return total_short_lca_lines


def write_shortened_bam(original_bam_path,short_lca_path,short_bam_path,stranded,minsimilarity,annotate_pmd,totallcalines): 
    # runs through the existing bam and the new short lca file at once, and writes only lines to the new bam which are represented in the short lca file
    # does two passes, the first of which makes a shortened header as well and adds the command str to the end of the bam header
    # also annotates with (correct!) pmd scores as it goes - the original pmdtools has a bug for single stranded libraries

    # now takes in minsimilarity as a percentage, and will keep reads w/ equal to or greater than NM flag to this percentage 

    if tqdm_imported:
        print(f"Writing a filtered bam file (a progress bar will initiate once the bam header has been written)...")
    else:
        print(f"Writing a filtered bam file...")

    # go and get get header lines in the OUTPUT lca, not the input (it will be 0, but just in case I modify code in the future)
    lcaheaderlines = 0
    with open(short_lca_path, 'r') as lcafile:
        for lcaline in lcafile:
            if "root" in lcaline:
                break
            lcaheaderlines += 1
    currentlcaline = lcaheaderlines

    with pysam.AlignmentFile(original_bam_path, "rb", check_sq=False, require_index=False) as infile, \
         pysam.AlignmentFile(short_bam_path, "wb", header=infile.header) as outfile, \
         open(short_lca_path, 'r') as shortlcafile:

        for _ in range(lcaheaderlines): 
            lcaline = next(shortlcafile)

        lcaline = next(shortlcafile)
        tab_split = lcaline.find('\t')
        colon_split = lcaline[:tab_split].rsplit(':', 3)[0]
        lcareadname = colon_split
    
        currentlymatching = False
        notdone = True

        try:
            bamread = next(infile)
        except StopIteration:
            notdone = False

        progress_bar = tqdm(total=totallcalines-lcaheaderlines, unit='lines', disable=not tqdm_imported) if tqdm_imported else None
        if progress_bar:
            update_interval = 100 # this is arbitrary. could also be e.g. (totallcalines // 1000)

        while notdone:
            if bamread.query_name == lcareadname:
                # copy this line and all the rest until you hit a nonmatching LCA line
                readlength = bamread.query_length # same read length for all the alignments
                similarity = 1 - bamread.get_tag('NM') / readlength # not the same NM for all the alignments
                if similarity >= minsimilarity:
                    if annotate_pmd:
                        pmd = get_pmd(bamread, stranded)
                        bamread.set_tag('DS','%.3f' % pmd)  
                    outfile.write(bamread) # write the read!
                currentlymatching = True
                while currentlymatching:
                    try:
                        bamread = next(infile)
                        if bamread.query_name == lcareadname:
                            similarity = 1 - bamread.get_tag('NM') / readlength
                            if similarity >= minsimilarity:
                                if annotate_pmd:
                                    pmd = get_pmd(bamread, stranded)
                                    bamread.set_tag('DS','%.3f' % pmd) # replace a tag if it's already there
                                outfile.write(bamread) # write the read! 
                        else:
                            currentlymatching = False
                    except StopIteration:
                        notdone = False
                        break
                try:
                    lcaline = next(shortlcafile)
                    tab_split = lcaline.find('\t')
                    lcareadname = lcaline[:tab_split].rsplit(':', 3)[0]
                    currentlcaline +=1 
                    if progress_bar and currentlcaline % update_interval == 0 :
                        progress_bar.update(update_interval) 
                except StopIteration:
                    notdone = False
            else:
                try:
                    bamread = next(infile)
                except StopIteration:
                    notdone = False
    if progress_bar:
        progress_bar.close()

    print("Wrote a filtered bam file. Done! \n") 

def get_mismatches(seq, cigar, md):  
    # parses a read, cigar and md string to determine mismatches and positions. 
    # does not output info on insertions/deletions but accounts for them.
    # thanks jonas oppenheimer who wrote half of this function :)

    # goes in two passes: once w/ cigar and once w/ md 

    cig = re.findall(r'\d+\D', cigar)
    md_list = re.compile(r'\d+|\^[A-Za-z]+|[A-Za-z]').findall(md)

    ref_seq = ''
    read_seq = ''
    query_pos = 0 # indexes the ref reconstruction
    read_pos = 0 # indexes the read reconstruction (which is the input read but with potential added "-"s if the ref has insertions)
    for x in cig:
        cat = x[-1]
        if cat in ['H', 'P']: # doesn't consume reference or query
            continue

        bases = int(x[:-1])

        if cat == 'S': # soft clip
            read_seq += seq[read_pos:read_pos + bases] # include the bases in the reconstructed read 
            # this includes soft clipped by default - edit this if you don't want to do this 
            continue

        elif cat in ['M', '=', 'X']: # match
            ref_seq += seq[query_pos:query_pos + bases]
            read_seq += seq[read_pos:read_pos + bases]
            query_pos += bases
            read_pos += bases

        elif cat in ['D', 'N']: # 'D' : the reference has something there and the read doesn't, but pad them both 
            ref_seq += 'N' * bases
            read_seq += '-' * bases

        elif cat == 'I': # I: the read has something there and the reference doesn't, but pad them both 
            read_seq += seq[read_pos:read_pos + bases] # 'N' * bases
            ref_seq += '-' * bases
            query_pos += bases
            read_pos += bases

        else:
            sys.exit("Error: You've got some strange cigar strings here.")
    
    ref_pos = 0
    read_pos = 0
    mismatch_list = [] # of format [ref, read, pos in alignment]
    for x in md_list:
        if x.startswith('^'): # this can be arbitrarily long (a ^ and then characters)
            num_chars_after_hat = len(x) -1 
            for i in range(0,num_chars_after_hat):
                currchar = x[i+1]
                refhere = currchar
                readhere = "-" 
                ref_pos += 1 # but not the read pos
                # don't need to append to mismatch list 
        else: 
            if x.isdigit(): # can be multiple digits 
                # you're a number or a letter
                # if you're a number, you're matching. skip ahead. don't need to add to the mismatch list. 
                ref_pos += int(x)
                read_pos += int(x)
            else: # it will only be one character at a time 
                refhere = x
                readhere = read_seq[ref_pos]
                # update ref_seq
                char_list = list(ref_seq)
                char_list[ref_pos] = x
                ref_seq = "".join(char_list)
                # moving on
                mismatch_list.append([refhere,readhere,read_pos +1]) # in genetics we are 1-based (a->g at position 1 means the actual first position, whereas python is 0-based) 
                if refhere is None or readhere is None:
                    print("Warning: There appears to be an inconsistency with seq " + seq + " and cigar " + cigar + " and md " + md)
                    break
                ref_pos += 1
                read_pos += 1
                # if you're a letter, you're mismatching. the letter is given in the ref. you can find the letter in the read in the seq[ref_pos]. 

    return [mismatch_list, ref_seq, read_seq] 

def mismatch_table(read,cigar,md,flagsum):
    # wrapper for get_mismatches that also reverse complements if needed, and mirrors around the middle of the read so you shouldn't have to keep the length

    readlength = len(read)

    # first parse the mismatches
    mms, refseq, readseq = get_mismatches(read,cigar,md)

    # some processing: first, figure out if it's forward or backwards
    # second field of a bam is called a flagsum and it parses like this (see eg bowtie2 manual)
    bin_flagsum = bin(flagsum)[::-1]
    bit_position = 4 #  2^4 = 16 this flag means it's aligned in reverse
    backwards = len(bin_flagsum) > bit_position and bin_flagsum[bit_position] == '1' 
    if backwards: # flip all the positions and reverse complement all the nucleotides. the read in the bam is reverse-complemented if aligned to the reverse strand. 
        complement = {'A': 'T', 'T': 'A', 'C': 'G', 'G': 'C'}
        mmsc = []
        for entry in range(0,len(mms)):
            new_entry = [
                complement.get(mms[entry][0]), 
                complement.get(mms[entry][1]), 
                readlength - mms[entry][2] +1
            ]
            mmsc.append(new_entry)
    else:
        mmsc = mms
    # now everything, EXCEPT unmerged but retained reverse mate pairs of paired end reads (which i should still try to catch later; to do), should be 5' -> 3' 

    for entry in range(0,len(mmsc)):
        pos = mmsc[entry][2]
        if pos > readlength/2:
            mmsc[entry][2] = -(readlength - pos +1)

    return mmsc

def rev_complement(seq):
    complement = {'A': 'T', 'T': 'A', 'C': 'G', 'G': 'C', 'N': 'N', '-': '-'}
    return ''.join(complement[base] for base in reversed(seq))

def get_rep_kmer(seq): # representative canonical kmer representation for counting, gets the lexicographical min of a kmer and its rev complement
    rep_kmer = min(seq,rev_complement(seq))
    return rep_kmer
 
def get_pmd(read, stranded):
    ### important!!! the original PMDtools implementation has a bug:
    # in lines 868 and 900 of the main python script, it multiplies the likelihood by the double-stranded models, and then there is an if statement that multiplies by the single-stranded models
    # resulting in totally incorrect pmd scores for single-stranded mode. the reimplementation here should be correct.
    
    # input is a pysam read object
    seq = read.query_sequence
    cigar = read.cigarstring
    md = read.get_tag('MD')
    rawphred = read.query_qualities
    flagsum = read.flag

    # set pmd score parameters . these are their original parameters, and i need to do some testing, but i think they are sensible enough in general. 
    P = 0.3
    C = 0.01
    pi = 0.001 

    # important note! in the PMDtools manuscript, they say "DZ=0" in the null model.
    # however, in the PMDtools code, Dz=0.001 in the null model.
    # here i am making the latter choice because i think it makes more biological sense.
    Dn = 0.001

    # find out if you're backwards
    bin_flagsum = bin(flagsum)[::-1]
    bit_position = 4 #  2^4 = 16 this flag means it's aligned in reverse
    backwards = len(bin_flagsum) > bit_position and bin_flagsum[bit_position] == '1' # backwards is a boolean 
    # do something if you are:
    mmsc, refseq, readseq = get_mismatches(seq, cigar, md)
    # adjust phred index if there are things happening in the read
    phred = [0 if base in ['-', 'N'] else rawphred.pop(0) for base in readseq]

    # run through both sequences to add up pmd likelihoods
    refseqlist = list(refseq)
    readseqlist = list(readseq)
    readlength = len(readseqlist)
    if backwards:
        refseqlist = rev_complement(refseqlist)
        readseqlist = rev_complement(readseqlist)
        phred = phred[::-1]
    pmd_lik = 1
    null_lik = 1
    pos = 0 # need a separate tracker to cope with indels. if there's a "-" in the read reconstruction because of an insertion in the ref, it should not count as a "position" in the read

    if stranded == "ss":
        for b in range(0,readlength):
            if readseqlist[b] == "-":
                continue # no pos +=1 
            # looking for c-> anything anywhere
            if refseqlist[b] == "C" and (readseqlist[b] == "T" or readseqlist[b] == "C"):
                # everything is relevant to 5 prime and 3 prime ends, get both distances
                epsilon = 1/3 * 10**(-phred[b] / 10)
                z = pos + 1 # pos 1 has distance 1, from pmd manuscript
                y = readlength - pos
                Dz = ((1-P)**(z-1))*P + C
                Dy = ((1-P)**(y-1))*P + C
                if readseqlist[b] == "T": # ss m
                    pmd_lik *= 1 - ((1-pi)*(1-epsilon)*(1-Dz)*(1-Dy) + (1-pi)*epsilon*Dz*(1-Dy) + (1-pi)*epsilon*Dy*(1-Dz) + pi*epsilon*(1-Dz)*(1-Dy))
                    null_lik *= 1 - ((1-pi)*(1-epsilon)*(1-Dn)*(1-Dn) + (1-pi)*epsilon*Dn*(1-Dn) + (1-pi)*epsilon*Dn*(1-Dn) + pi*epsilon*(1-Dn)*(1-Dn))                 
                if readseqlist[b] == "C": # ss m
                    pmd_lik *= (1-pi)*(1-epsilon)*(1-Dz)*(1-Dy) + (1-pi)*epsilon*Dz*(1-Dy) + (1-pi)*epsilon*Dy*(1-Dz) + pi*epsilon*(1-Dz)*(1-Dy)
                    null_lik *= (1-pi)*(1-epsilon)*(1-Dn)*(1-Dn) + (1-pi)*epsilon*Dn*(1-Dn) + (1-pi)*epsilon*Dn*(1-Dn) + pi*epsilon*(1-Dn)*(1-Dn)
                pos +=1 

    if stranded == "ds":
        for b in range(0,readlength):
            if readseqlist[b] == "-":
                continue # no pos +=1 
            if refseqlist[b] == "C" and (readseqlist[b] == "T" or readseqlist[b] == "C"):
                # get distance and stuff to 5 prime end
                epsilon = 1/3 * 10**(-phred[b] / 10)
                z = pos + 1   # 5 prime
                Dz = ((1-P)**(z-1))*P + C

                if readseqlist[b] == "T": # ds mm 
                    pmd_lik *=  1 - ((1-pi)*(1-epsilon)*(1-Dz) + (1-pi)*epsilon*Dz + pi*epsilon*(1-Dz)) 
                    null_lik *= 1 - ((1-pi)*(1-epsilon)*(1-Dn) + (1-pi)*epsilon*Dn + pi*epsilon*(1-Dn)) 
                
                if readseqlist[b] == "C": # ds match
                    pmd_lik *= (1-pi)*(1-epsilon)*(1-Dz) + (1-pi)*epsilon*Dz + pi*epsilon*(1-Dz)
                    null_lik *= (1-pi)*(1-epsilon)*(1-Dn) + (1-pi)*epsilon*Dn + pi*epsilon*(1-Dn)

            if refseqlist[b] == "G" and (readseqlist[b] == "A" or readseqlist[b] == "G"):
                # get distance and stuff to 3 prime end
                epsilon = 1/3 * 10**(-phred[b] / 10) # phred score 30 gives an error rate of 0.001 (then * 1/3)
                z = readlength - pos  # 3 prime
                Dz = ((1-P)**(z-1))*P + C
                if readseqlist[b] == "A": # ds mm
                    pmd_lik *=  1 - ((1-pi)*(1-epsilon)*(1-Dz) + (1-pi)*epsilon*Dz + pi*epsilon*(1-Dz)) 
                    null_lik *= 1 - ((1-pi)*(1-epsilon)*(1-Dn) + (1-pi)*epsilon*Dn + pi*epsilon*(1-Dn)) 
                if readseqlist[b] == "G": # ds m
                    pmd_lik *= (1-pi)*(1-epsilon)*(1-Dz) + (1-pi)*epsilon*Dz + pi*epsilon*(1-Dz)
                    null_lik *= (1-pi)*(1-epsilon)*(1-Dn) + (1-pi)*epsilon*Dn + pi*epsilon*(1-Dn)
            
            pos +=1 
    
    if pmd_lik == 0 or null_lik == 0:
        pmd_score = 0
    else:
        pmd_score = math.log(pmd_lik/null_lik)

    return pmd_score

def line_count(file_path): # just a wc -l wrapper
    result = subprocess.run(['wc', '-l', file_path], stdout=subprocess.PIPE, text=True)
    line_count = int(result.stdout.split()[0])
    return line_count

def gini_index(seq, k):
    kmer_counts = {}
    for i in range(len(seq) - k + 1):
        kmer = seq[i:i + k]
        if not all(base in {'A', 'C', 'T', 'G'} for base in kmer):
            print(f"Warning: Skipping k-mer calculations for a read with non-ACGT characters.")
            continue 
        if kmer in kmer_counts:
            kmer_counts[kmer] += 1
        else:
            kmer_counts[kmer] = 1
    counts = sorted(kmer_counts.values())
    n = len(counts)
    total_sum = sum(counts)
    pairwise_differences = 0
    for i in range(n):
        for j in range(i + 1, n):
            pairwise_differences += abs(counts[i] - counts[j])
    gini = pairwise_differences / (n * total_sum) 
    return gini

def get_hll_info(seq,k):
    # output to dump into hll objects
    rep_kmers = []
    total_kmers = 0
    if len(seq) > k:
        for i in range(len(seq) - k + 1):
            kmer = seq[i:i + k]
            if not all(base in {'A', 'C', 'T', 'G'} for base in kmer):         
                print(f"Warning: Skipping k-mer calculations for a read with non-ACGT characters.")       
                continue # skip this k-mer, non ACTG characters are not allowed
            else:
                rep_kmers.append(get_rep_kmer(kmer))
                total_kmers +=1 
    else:
        print(f"Warning: One of your reads is shorter than kn.")
    return rep_kmers, total_kmers


def gather_subs_and_kmers(bamfile_path, lcafile_path, kr, kn, upto,stranded):
    print("\n Gathering substitution and kmer metrics per node...")

    lcaheaderlines = 0
    with open(lcafile_path, 'r') as lcafile:
        for lcaline in lcafile:
            if "root" in lcaline:
                break
            lcaheaderlines += 1

    # initialize 
    node_data = {}
    bamfile = pysam.AlignmentFile(bamfile_path, "rb") 
    lcafile = open(lcafile_path, 'r')
    oldreadname = ""
    oldmd = ""
    oldcigar = ""
    oldflagsum = ""
    nodestodumpinto = []
    num_alignments = 0
    currentsubdict = {}
    nms = 0 
    pmdsover2 = 0
    pmdsover4 = 0
    are_pmds_in_the_bam = True # just assume true, then set to false quickly below if you notice otherwise 
    ctp1 = 0
    ctm1 = 0
    gam1 = 0
    div = 0

    currentlcalinenum = 0
    if tqdm_imported:
        totallcalines = line_count(lcafile_path)
        # should be super fast compared to anything else; probably worth it to initiate a progress bar.
        progress_bar = tqdm(total=totallcalines-lcaheaderlines, unit='lines', disable=not tqdm_imported) if tqdm_imported else None
        if progress_bar:
            update_interval = 100 # this is arbitrary ofc. could also be e.g. (totallcalines // 1000) 

    for _ in range(lcaheaderlines +1):
        currentlcaline = next(lcafile) 

    for read in bamfile:

        # get the basic info for this read
        readname = read.query_name

        # immediately find out if it's a new read. if so, you JUST finished the last read, so do a bunch of stuff for it. 
        if readname != oldreadname and oldreadname != "":

            # do k-mer things for this read
            # first get gini index for this read
            gini = gini_index(seq,kr) 
            # then get all the rep kmers to dump into the hyperloglog for each relevant node below
            rep_kmers, total_kmers = get_hll_info(seq,kn)

            # get the lca entry and nodes we wanna update
            lcaentry = currentlcaline.split('\t')
            fields = lcaentry[1:]
            nodestodumpinto = []
            for i in range(len(fields)):
                if upto in fields[i]:
                    # gotta be the last one! 
                    nodestodumpinto.append(fields[i].split(':')[0])
                    break
                nodestodumpinto.append(fields[i].split(':')[0])
                
            colon_split = lcaentry[0].rsplit(':', 3)[0]
            lcareadname = colon_split

            if oldreadname != lcareadname:
                print("Warning: There is a mismatch between your lca and bam files at read " + oldreadname + " in the bam and " + lcareadname + " in the lca. ")
                print("This mismatch could have some bad downstream implications, and it would be best to find out what's causing it, fix it, and re-run. Perhaps you have some reads in one file but not in the other? \n")
                break

            # now update everything to all the relevant nodes
            for node in nodestodumpinto:
                if node not in node_data:
                    # that's ok! add it. 
                    if are_pmds_in_the_bam:
                        node_data[node] = {'total_reads': 0,'pmdsover2': 0, 'pmdsover4': 0, 'meanlength': 0, 'total_alignments': 0, 
                                       'ani': 0, 'avgperreadgini' : 0, 'avggc': 0, 'tax_path' : "", 'subs': {},
                                       'dp1' : 0, 'dm1' : 0, 'div': 0, 'hll': hyperloglog.HyperLogLog(0.01), 'totalkmers' : 0 }
                    else:
                        node_data[node] = {'total_reads': 0,'meanlength': 0, 'total_alignments': 0, 
                                       'ani': 0, 'avgperreadgini' : 0, 'avggc': 0, 'tax_path' : "", 'subs': {},
                                       'dp1' : 0, 'dm1' : 0, 'div': 0, 'hll': hyperloglog.HyperLogLog(0.01), 'totalkmers' : 0 }
                node_data[node]['meanlength'] = ((node_data[node]['meanlength'] * node_data[node]['total_reads']) + readlength) / (node_data[node]['total_reads'] + 1)
                node_data[node]['avgperreadgini'] = ( (node_data[node]['avgperreadgini'] * node_data[node]['total_reads']) + gini) / (node_data[node]['total_reads'] + 1)
                ani_for_this_read = (readlength - nms/num_alignments)/readlength 
                node_data[node]['ani'] = (ani_for_this_read + node_data[node]['ani'] * node_data[node]['total_reads']) / (node_data[node]['total_reads'] + 1)
                gc_content_for_this_read = (seq.count('C') + seq.count('G')) / readlength
                node_data[node]['avggc'] = ((node_data[node]['avggc'] * node_data[node]['total_reads']) + gc_content_for_this_read) / (node_data[node]['total_reads'] + 1)
                node_data[node]['total_alignments'] += num_alignments
                if are_pmds_in_the_bam:
                    node_data[node]['pmdsover2'] += pmdsover2 / num_alignments
                    node_data[node]['pmdsover4'] += pmdsover4 / num_alignments
                ctp1 = currentsubdict.get("['C', 'T', 1]", 0) # c -> t on the pos 1 
                ctm1 = currentsubdict.get("['C', 'T', -1]", 0) # c -> t on the pos 1 
                gam1 = currentsubdict.get("['G', 'A', -1]", 0) # c -> t on the pos 1 
                # ok, so on joshua kapp's suggestion, i used to only add to dp1 and dm1 if all the alignments for that read had a c>t, which does make sense.
                # but then it doesn't match the damage plots and that bugs me to no end, so i went back to adding the proportion of reads which had a c>t.
                # an if ctp1 == num_alignments:  will do it josh's way 
                node_data[node]['dp1'] = ((node_data[node]['dp1'] * node_data[node]['total_reads']) + (ctp1/num_alignments) ) / (node_data[node]['total_reads'] + 1)
                if stranded == "ss":
                    node_data[node]['dm1'] = ((node_data[node]['dm1'] * node_data[node]['total_reads']) + (ctm1/num_alignments) )  / (node_data[node]['total_reads'] + 1)
                if stranded == "ds":
                    node_data[node]['dm1'] = ((node_data[node]['dm1'] * node_data[node]['total_reads']) + (gam1/num_alignments) ) / (node_data[node]['total_reads'] + 1)

                # update hyperloglogs
                for kmer in rep_kmers:
                    node_data[node]['hll'].add(kmer)
                node_data[node]['totalkmers'] += total_kmers

                # updates substitution tables similarly
                other_sub_count = 0
                if currentsubdict:
                    for sub, count in currentsubdict.items():
                        if not ((sub[0] == 'C' and sub[1] == 'T') or (sub[0] == 'G' and sub[1] == 'A')):
                            other_sub_count += count # don't include c>t or g>a in any case, regardless of library 
                        if sub in node_data[node]['subs']: 
                            node_data[node]['subs'][sub] += count / num_alignments
                        else:
                            node_data[node]['subs'][sub] = count / num_alignments # so, this can be up to 1 per node. 
                # last bits to update
                div = other_sub_count / (num_alignments * readlength)
                node_data[node]['div'] = ((node_data[node]['div'] * node_data[node]['total_reads']) + div ) / (node_data[node]['total_reads'] + 1)
                # add the tax path if it's not already there
                if node_data[node]['tax_path'] == "":
                    lca_index = next(i for i, entry in enumerate(lcaentry) if entry.startswith(node))
                    tax_path = ','.join(lcaentry[lca_index:]).replace('\n','')
                    node_data[node]['tax_path'] = tax_path
                
                # only at the end should you update total reads 
                node_data[node]['total_reads'] += 1

            # move on to the next lca entry. re initialize a bunch of things here 
            oldreadname = readname
            oldmd = ""
            oldcigar = ""
            oldflagsum = ""
            currentlcaline = next(lcafile)
            currentlcalinenum += 1 
            if progress_bar and (currentlcalinenum % update_interval == 0) :
                progress_bar.update(update_interval)  # Update the progress bar
            currentsubdict = {}
            num_alignments = 0
            nms = 0
            if are_pmds_in_the_bam:
                pmdsover2 = 0
                pmdsover4 = 0
            ctp1 = 0
            ctm1 = 0
            gam1 = 0
            div = 0 

        # now for the current alignment.
        # the following might change for different alignments of the same read: 
        seq = read.query_sequence
        readlength = len(seq)
        cigar = read.cigarstring
        md = read.get_tag('MD')
        nms += read.get_tag('NM')
        if are_pmds_in_the_bam: 
            try:
                pmd = float(read.get_tag('DS'))
            except KeyError:
                pmd = 0
            if(pmd>2):
                pmdsover2 += 1 
            if(pmd>4):
                pmdsover4 += 1
        flagsum = read.flag
        num_alignments += 1 

        # go and get the mismatch table for this read if the name/md/cigar/flagsum is different to before (this is expensive, so avoid it when possible)
        if (readname != oldreadname) or (cigar != oldcigar) or (md != oldmd) or (flagsum != oldflagsum):
            subs = mismatch_table(seq,cigar,md,flagsum) 
            oldcigar = cigar; oldmd = md; oldflagsum = flagsum
        for sub in subs:
            key = "".join(str(sub))
            if key in currentsubdict:
                currentsubdict[key] +=1
            else:
                currentsubdict[key] = 1

        # quick catch for the starting read; check if the first read (and then presumably the whole bam) has a pmd score 
        if oldreadname == "":
            oldreadname = readname
            try:
                pmd = float(read.get_tag('DS'))
            except KeyError:
                are_pmds_in_the_bam = False

    if progress_bar:
        progress_bar.close()

    print("\nGathered substitution and kmer data for " + str(len(node_data)) + " taxonomic nodes. Now sorting and writing output files... ")

    bamfile.close() 
    lcafile.close()

    return node_data, are_pmds_in_the_bam

def format_subs(subs, nreads):
    formatted_subs = []
    
    for key, value in subs.items():
        # extract position and check if it is within the range -15 to 15 (more is unnecessary for damage)
        # easy to remove the condition if you want to write all of the subs though!
        parts = key.strip("[]").replace("'", "").split(", ")
        pos = int(parts[2])
        if (-15 <= pos <= 15) and (parts[0] in {'A', 'C', 'T', 'G'}) and (parts[1] in {'A', 'C', 'T', 'G'}):
            formatted_key = "".join(parts)
            formatted_value = round(value / nreads, 3)  
            formatted_subs.append((pos, f"{formatted_key}:{formatted_value}"))
            formatted_subs.sort(key=lambda x: (x[0] > 0, (x[0])))

    return " ".join(sub[1] for sub in formatted_subs)


def parse_and_write_node_data(nodedata, stats_path, subs_path, stranded, pmds_in_bam):
    # parses a dictionary where keys are node tax ids, and entries are total_reads, meanlength, total_alignments, etc

    statsfile = open(stats_path, 'w', newline='')
    subsfile = open(subs_path, 'w', newline='')
    if pmds_in_bam:
        header = ['TaxNodeID', 'TaxName', 'TotalReads', 'ND+1', 'ND-1', 'UniqueKmers', 'Duplicity', 
              'MeanLength', 'Div', 'ANI','PerReadKmerGI', 'AvgGC', 'Damage+1', 'Damage-1','TotalAlignments', 
                'PMDsover2', 'PMDSover4','taxpath'] 
    else:
            header = ['TaxNodeID', 'TaxName', 'TotalReads', 'ND+1', 'ND-1', 'UniqueKmers', 'Duplicity', 
              'MeanLength', 'Div', 'ANI','PerReadKmerGI', 'AvgGC', 'Damage+1', 'Damage-1','TotalAlignments','taxpath'] 
    statsfile.write('\t'.join(header) + '\n')
    writer = csv.writer(statsfile, delimiter='\t', quotechar='"', quoting=csv.QUOTE_NONNUMERIC)
    subswriter = csv.writer(subsfile, delimiter='\t', quotechar='"', quoting=csv.QUOTE_NONE)

    rows = []
    subsrows = {}

    for node in nodedata:
        tn = nodedata[node]
        
        # get formatted subs
        fsubs = format_subs(tn['subs'], tn['total_reads'])

        # number of unique k-mers approximated by the hyperloglog algorithm
        numuniquekmers = len(tn['hll'])
        duplicity = tn['totalkmers'] / numuniquekmers

        taxname = tn['tax_path'].split(",")[0].split(":")[1]

        # get normalized +1 and -1 damage frequencies
        dp1n = tn['dp1'] - tn['div']
        dm1n = tn['dm1'] - tn['div']

        # write 
        if pmds_in_bam:
            row = [int(node), taxname, tn['total_reads'],
               round(dp1n,4), round(dm1n,4), numuniquekmers, round(duplicity,2),
               round(tn['meanlength'], 2), round(tn['div'], 4), round(tn['ani'], 4), 
               round(tn['avgperreadgini'], 4), 
               round(tn['avggc'], 3), round(tn['dp1'],4), 
               round(tn['dm1'],4), 
               tn['total_alignments'], 
               round(tn['pmdsover2']/tn['total_reads'],3), round(tn['pmdsover4']/tn['total_reads'], 3),  
               tn['tax_path']] 
        else:
            row = [int(node), taxname, tn['total_reads'],
               round(dp1n,4), round(dm1n,4), numuniquekmers, round(duplicity,2),
               round(tn['meanlength'], 2), round(tn['div'], 4), round(tn['ani'], 4), 
               round(tn['avgperreadgini'], 4), 
               round(tn['avggc'], 3), round(tn['dp1'],4), 
               round(tn['dm1'],4), 
               tn['total_alignments'], 
               tn['tax_path']] 
        rows.append(row)

        subsrows[int(node)] = [int(node), taxname, fsubs]
    
    rows.sort(key=lambda x: x[2], reverse=True)

    for row in rows:
        writer.writerow(row)
    
    for row in rows:
        subswriter.writerow(subsrows[row[0]])

    statsfile.close()
    subsfile.close()

    print("Wrote final stats and subs files. Done!")


def extract_reads(in_lca, in_bam, out_bam, tax):
    # a very basic wrapper for grep / awk / samtools view.
    # writes a temp file with readnames, because that's what samtools view wants. 
    # actually i did try to do this more cleverly, using the known ordering of bam and lca, but it was slower.
    tax_pattern = f"\\t{tax}:" if tax.isdigit() else tax
    tmp_file = f"{out_bam}.readnames.tmp"
    grep_command = (
        f"grep '{tax_pattern}' {in_lca} | awk -F'\\t' '{{print $1}}' "
        f"| awk -F':' '{{for(i=1; i<=NF-3; i++) printf $i (i<NF-3 ? \":\" : \"\\n\")}}' > {tmp_file}"
    )
    result = subprocess.run(grep_command, shell=True)
    if result.returncode != 0:
        print(f"No matches found for keyword: {tax} or grep/awk command failed.")
        return
    pysam.view("-N", tmp_file, "-b", in_bam, "-o", out_bam, catch_stdout=False)
    try:
        os.remove(tmp_file)
    except OSError as e:
        print(f"Error: {tmp_file} : {e.strerror}")


def make_damage_plot(subs,tax,plotfile):
    # just damage from the subs file; should be super fast 

    if matplotlib_imported == False:
        print(f"Error: Cannot find matplotlib library for plotting. Try: pip install matplotlib")
        return

    with open(subs, 'r') as f:
        file_content = f.readlines()

    tax_pattern = f"^{tax}\\t" if tax.isdigit() else tax
    matched_line = [line for line in file_content if re.match(tax_pattern, line)]

    if len(matched_line) == 0:
        matched_line = [line for line in file_content if tax.lower() in line.lower()]  # capitalization error?
        if len(matched_line) == 1:
            taxstring = matched_line[0].split('\t')[:2]
            print(f"There was no exact match for your input tax string in the subs file, but a close match was found and will be used: {taxstring}")
        else:
            raise ValueError(f"No matching lines found for the given tax strings in the subs file.")
    elif len(matched_line) > 1:
        raise ValueError("More than one matching line found. Please be more specific, e.g. by using a tax id instead of a name.")

    split_line = matched_line[0].split('\t')
    tax_id, tax_name, data_part = split_line[0], split_line[1], split_line[2]
    data_items = data_part.split()

    positions = list(range(-15, 0)) + list(range(1, 16))
    values = {'Other': [0] * 30, 'CT': [0] * 30, 'GA': [0] * 30}

    for item in data_items:
        parts = item.split(":")
        sub_type = parts[0]  
        freq = float(parts[1])  
        if "None" not in sub_type:  
            sub_key = sub_type[:2]  
            pos = int(sub_type[2:]) 
            if -15 <= pos <= 15:
                if sub_key in values:
                    values[sub_key][positions.index(pos)] = freq
                else:
                    values['Other'][positions.index(pos)] += freq  #

    max_y = max(max(values['Other']), max(values['CT']), max(values['GA'])) * 1.1

    plt.figure(figsize=(10, 5))
    color_palette = {'Other': '#009E73', 'CT': '#F8766D', 'GA': '#56B4E9'}  # colorblind friendly!

    ax1 = plt.subplot(1, 2, 1)
    for key in values:
        ax1.plot([str(pos) for pos in positions if pos > 0],
                 [values[key][i] for i, pos in enumerate(positions) if pos > 0],
                 label=key, color=color_palette[key], linewidth=2) 
    ax1.set_ylim(0, max_y)
    ax1.set_xlabel('Position')
    ax1.set_ylabel('Frequency')
    ax1.yaxis.set_ticks_position('left') 
    ax1.tick_params(right=False)

    ax2 = plt.subplot(1, 2, 2)
    for key in values:
        ax2.plot([str(pos) for pos in positions if pos < 0],
                 [values[key][i] for i, pos in enumerate(positions) if pos < 0],
                 label=key, color=color_palette[key], linewidth=2)  
    ax2.set_ylim(0, max_y)  
    ax2.set_xlabel('Position')
    ax2.yaxis.set_ticks_position('right')  
    ax2.tick_params(left=False)
    ax2.legend(labels=['Other', 'C to T', 'G to A'], loc='upper left')

    plt.suptitle(f'Damage Plot for {tax_name} (Tax ID: {tax_id})')
    plt.tight_layout(rect=[0, 0, 1, 0.95])

    file_extension = os.path.splitext(plotfile)[1].lower()
    if file_extension == '.pdf':
        plt.savefig(plotfile, format='pdf')
    else:
        if file_extension != '.png':
            print("Warning: Invalid plot file suffix. Your plot file is being saved in .png format with the filename you requested.")
        plt.savefig(plotfile) 

    plt.close()

def make_baminfo_plot(in_bam, plotfile):

    if matplotlib_imported == False:
        print(f"Error: Cannot find matplotlib library for plotting. Try: pip install matplotlib")
        return

    bamfile = pysam.AlignmentFile(in_bam, "rb")
    
    mismatch_counts = {}  
    read_length_counts = {}  
    
    current_readname = None
    mismatch_bins = {}  
    alignment_count = 0
    current_read_length = 0
    total_reads = 0

    for read in bamfile:
        readname = read.query_name

        if readname != current_readname:
            if current_readname is not None:
                for mismatch_bin, count in mismatch_bins.items():
                    fraction = count / alignment_count
                    if mismatch_bin not in mismatch_counts:
                        mismatch_counts[mismatch_bin] = fraction
                    else:
                        mismatch_counts[mismatch_bin] += fraction

                if current_read_length not in read_length_counts:
                    read_length_counts[current_read_length] = 1
                else:
                    read_length_counts[current_read_length] += 1

                total_reads += 1

            current_readname = readname
            mismatch_bins = {}  
            alignment_count = 0
            current_read_length = read.query_length

        mismatches = read.get_tag("NM") if read.has_tag("NM") else 0
        if mismatches not in mismatch_bins:
            mismatch_bins[mismatches] = 1
        else:
            mismatch_bins[mismatches] += 1

        alignment_count += 1

    if current_readname is not None:
        for mismatch_bin, count in mismatch_bins.items():
            fraction = count / alignment_count
            if mismatch_bin not in mismatch_counts:
                mismatch_counts[mismatch_bin] = fraction
            else:
                mismatch_counts[mismatch_bin] += fraction

        if current_read_length not in read_length_counts:
            read_length_counts[current_read_length] = 1
        else:
            read_length_counts[current_read_length] += 1

        total_reads += 1

    bamfile.close()

    x_mismatch_values = sorted(mismatch_counts.keys())
    y_mismatch_values = [mismatch_counts[x] / total_reads for x in x_mismatch_values]

    x_readlength_values = sorted(read_length_counts.keys())
    y_readlength_values = [read_length_counts[x] for x in x_readlength_values]

    plt.figure(figsize=(10, 5))

    plt.subplot(1, 2, 1)
    plt.plot(x_mismatch_values, y_mismatch_values, marker='o', linestyle='-', color='#009E73')
    plt.xlabel("Number of mismatches")
    plt.ylabel("Frequency (per read)")
    plt.title("Mismatch Frequency Plot")
    plt.xticks(x_mismatch_values)
    plt.grid(False)

    plt.subplot(1, 2, 2)
    plt.plot(x_readlength_values, y_readlength_values, marker='o', linestyle='-', color='#CC79A7')
    plt.xlabel("Read length")
    plt.ylabel("Number of reads")
    plt.title("Read Length Distribution")
    plt.grid(False)

    plt.tight_layout()

    file_extension = os.path.splitext(plotfile)[1].lower()
    if file_extension == '.pdf':
        plt.savefig(plotfile, format='pdf')
    else:
        if file_extension != '.png':
            print("Warning: Invalid plot file suffix. Your plot file is being saved in .png format, using the filename you specified.")
        plt.savefig(plotfile)

    plt.close()

def shrink(args):
    shortlcalines = write_shortened_lca(args.in_lca, args.out_lca, args.upto, args.mincount, args.exclude_keywords, args.exclude_under)
    write_shortened_bam(args.in_bam, args.out_lca, args.out_bam, args.stranded, args.minsim, args.annotate_pmd, shortlcalines)

def compute(args):
    nodedata, pmds_in_bam = gather_subs_and_kmers(args.in_bam, args.in_lca, kr=args.kr, kn=args.kn, upto=args.upto, stranded=args.stranded)
    parse_and_write_node_data(nodedata, args.out_stats, args.out_subs, args.stranded, pmds_in_bam)  

def extract(args):
    extract_reads(args.in_lca, args.in_bam, args.out_bam, args.keyword)

def plotdamage(args):
    make_damage_plot(args.in_subs, args.tax, args.outplot)

def plotbaminfo(args):
    make_baminfo_plot(args.in_bam, args.outplot)

def main():

    # Initialize
    parser = argparse.ArgumentParser(
        description="Bamdam processes ancient environmental DNA bam and lca files. Type bamdam [command] -h for more detailed help regarding a specific command.")
    
    subparsers = parser.add_subparsers(dest="command", required=True)

    # Shrink
    parser_shrink = subparsers.add_parser('shrink', help="Filter the BAM and LCA files.")
    parser_shrink.add_argument("--in_lca", type=str, required=True, help="Path to the input LCA file (required)")
    parser_shrink.add_argument("--in_bam", type=str, required=True, help="Path to the input (read-sorted) BAM file (required)")
    parser_shrink.add_argument("--out_lca", type=str, required=True, help="Path to the short output LCA file (required)")
    parser_shrink.add_argument("--out_bam", type=str, required=True, help="Path to the short output BAM file (required)")
    parser_shrink.add_argument("--stranded", type=str, required=True, help="Either ss for single stranded or ds for double stranded (required)")
    parser_shrink.add_argument("--mincount", type=int, default=5, help="Minimum read count to keep a node (default: 5)")
    parser_shrink.add_argument("--upto", type=str, default="family", help="Keep nodes up to and including this tax threshold; use root to disable (default: family)")
    parser_shrink.add_argument("--minsim", type=float, default=0.9, help="Minimum similarity to reference to keep an alignment (default: 0.9)")
    parser_shrink.add_argument("--exclude_keywords", type=str, nargs='+', default=[], help="Keyword(s) to exclude when filtering (default: none)")
    parser_shrink.add_argument("--exclude_keyword_file", type=str, default=None, help="File of keywords to exclude when filtering, one per line (default: none)")
    parser_shrink.add_argument("--exclude_under", action='store_true', help="Do not keep nodes below criteria-meeting nodes unless they also meet criteria themselves (default: not set)")
    parser_shrink.add_argument("--annotate_pmd", action='store_true', help="Annotate output bam file with PMD tags  (default: not set)")
    parser_shrink.set_defaults(func=shrink) 

    # Compute
    parser_compute = subparsers.add_parser('compute', help="Compute subs and stats files.")
    parser_compute.add_argument("--in_bam", type=str, required=True, help="Path to the BAM file (required)")
    parser_compute.add_argument("--in_lca", type=str, required=True, help="Path to the LCA file (required)")
    parser_compute.add_argument("--out_stats", type=str, required=True, help="Path to the output stats file (required)")
    parser_compute.add_argument("--out_subs", type=str, required=True, help="Path to the output subs file (required)")
    parser_compute.add_argument("--stranded", type=str, required=True, help="Either ss for single stranded or ds for double stranded (required)")
    parser_compute.add_argument("--kn", type=int, default=29, help="Value of k for per-node counts of unique k-mers and duplicity (default: 29)")
    parser_compute.add_argument("--kr", type=int, default=5, help="Value of k for per-read kmer complexity calculation (default: 5)")
    parser_compute.add_argument("--upto", type=str, default="family", help="Keep nodes up to and including this tax threshold; use root to disable (default: family)")
    parser_compute.set_defaults(func=compute)

    # Extract
    parser_extract = subparsers.add_parser('extract', help="Extract alignments of reads containing a keyword in an associated lca file.")
    parser_extract.add_argument("--in_bam", type=str, required=True, help="Path to the BAM file (required)")
    parser_extract.add_argument("--in_lca", type=str, required=True, help="Path to the LCA file (required)")
    parser_extract.add_argument("--out_bam", type=str, required=True, help="Path to the filtered BAM file (required)")
    parser_extract.add_argument("--keyword", type=str, required=True, help="Keyword or phrase to filter for, e.g. a taxonomic node ID (required)")
    parser_extract.set_defaults(func=extract)

    # Plot damage
    parser_plotdamage = subparsers.add_parser('plotdamage', help="Produces a postmortem damage plot for a specified taxonomic node using the subs file.")
    parser_plotdamage.add_argument("--in_subs", type=str, required=True, help="Path to the subs file produced by bamdam compute (required)")
    parser_plotdamage.add_argument("--tax", type=str, required=True, help="Taxonomic node ID (required)")
    parser_plotdamage.add_argument("--outplot", type=str, default="damage_plot.png", help="Filename for the output plot, ending in .png or .pdf (default: damage_plot.png)")
    parser_plotdamage.set_defaults(func=plotdamage)

    # Plot bam info
    parser_plotbaminfo = subparsers.add_parser('plotbaminfo', help="Produces a mismatch and read length distribution plot for an input bam.")
    parser_plotbaminfo.add_argument("--in_bam", type=str, required=True, help="Path to the BAM file (required)")
    parser_plotbaminfo.add_argument("--outplot", type=str, default="baminfo_plot.png", help="Filename for the output plot, ending in .png or .pdf (default: baminfo_plot.png)")
    parser_plotbaminfo.set_defaults(func=plotbaminfo)

    if len(sys.argv)==1:
        parser.print_help()
        sys.exit(1)

    args = parser.parse_args()

    if args.command == None:
        parser.print_help()
        sys.exit(1)

    # Validation checks
    if hasattr(args, 'stranded') and args.stranded not in ["ss", "ds"]:
        parser.error(f"Invalid value for stranded: {args.stranded}. Valid values are 'ss' or 'ds'.")
    if hasattr(args, 'mincount') and not isinstance(args.mincount, int):
        parser.error(f"Invalid integer value for mincount: {args.mincount}")
    if hasattr(args, 'kr') and (not isinstance(args.kr, int) or not isinstance(args.kr, int) or args.kr > 30):
        parser.error(f"Invalid integer value for kr : {args.kr} (max 29, and that is much higher than recommended in any case)")
    if hasattr(args, 'kn') and (not isinstance(args.kn, int) or not isinstance(args.kn, int) or args.kn > 50):
        parser.error(f"Invalid integer value for kn : {args.kn} (max 49, and that is much higher than recommended in any case)")
    if hasattr(args, 'upto') and not re.match("^[a-z]+$", args.upto):
        parser.error(f"Invalid value for upto: {args.upto}. Must be a string of only lowercase letters.")
    if hasattr(args, 'minsim') and not isinstance(args.minsim, float):
        parser.error(f"Invalid float value for minsim: {args.minsim}")
    if hasattr(args, 'in_lca') and not os.path.exists(args.in_lca):
        parser.error(f"Input LCA path does not exist: {args.in_lca}")
    if hasattr(args, 'in_bam') and not os.path.exists(args.in_bam):
        parser.error(f"Input BAM path does not exist: {args.in_bam}")
    if hasattr(args, 'upto') and args.upto=="clade":
        parser.error(f"Clade is not a valid taxonomic level in bamdam because there can be multiple clades in one taxonomic path.")
    if hasattr(args, 'upto') and args.upto != args.upto.lower():
        parser.warning(f"Warning: {args.upto} as provided is not in lowercase. Converting to lowercase and moving on.")
        args.upto = args.upto.lower()
    
    if hasattr(args, 'in_bam') and args.command != "plotbaminfo":
        sortorder = get_sorting_order(args.in_bam)
        if sortorder != "queryname":
            print("Error: Your bam file does not appear to be read-sorted. Please try again with it once it has been read-sorted (samtools sort -n), which should be the same order as your lca file. \n")
            exit(-1) # remove this line to force continue

    # deal with exclude keywords
    if hasattr(args, 'exclude_keywords') or hasattr(args, 'exclude_keyword_file'):
        if args.exclude_keywords and args.exclude_keyword_file:
            parser.error("Please only provide one of --exclude_keywords or --exclude_keyword_file, not both.")
        exclude_keywords = args.exclude_keywords
        if type(exclude_keywords) != type([]):
            exclude_keywords = [exclude_keywords]
        if args.exclude_keyword_file:
            if not os.path.exists(args.exclude_keyword_file):
                parser.error(f"Exclude keyword file path does not exist: {args.exclude_keyword_file}")
            with open(args.exclude_keyword_file, 'r') as f: # remove quotation marks if they're in the file 
                exclude_keywords.extend([line.strip().strip('"').strip("'") for line in f if line.strip()])
    
    if args.command == 'shrink':
        print("Hello! You are running bamdam shrink with the following arguments:")
        print(f"in_lca: {args.in_lca}")
        print(f"in_bam: {args.in_bam}")
        print(f"out_lca: {args.out_lca}")
        print(f"out_bam: {args.out_bam}")
        print(f"stranded: {args.stranded}")
        print(f"mincount: {args.mincount}")
        print(f"upto: {args.upto}")
        print(f"minsim: {args.minsim}")
        if hasattr(args, 'exclude_keyword_file') and args.exclude_keyword_file:
            print(f"exclude_keywords: loaded from {args.exclude_keyword_file}")
        if hasattr(args, 'exclude_keywords') and args.exclude_keywords:
            print(f"exclude_keywords: {args.exclude_keywords}")
        if hasattr(args, 'exclude_keyword_file') or hasattr(args, 'exclude_keywords'):
            print(f"exclude_under: {args.exclude_under}")
        if hasattr(args, 'annotate_pmd') and args.annotate_pmd:
            print(f"annotate_pmd: {args.annotate_pmd}")

    elif args.command == 'compute':
        print("Hello! You are running bamdam compute with the following arguments:")
        print(f"in_bam: {args.in_bam}")
        print(f"in_lca: {args.in_lca}")
        print(f"out_stats: {args.out_stats}")
        print(f"out_subs: {args.out_subs}")
        print(f"stranded: {args.stranded}")
        print(f"kr: {args.kr}")
        print(f"kn: {args.kn}")
        print(f"upto: {args.upto}")

    elif args.command == 'extract':
        print("Hello! You are running bamdam extract with the following arguments:")
        print(f"in_bam: {args.in_bam}")
        print(f"in_lca: {args.in_lca}")
        print(f"out_bam: {args.out_bam}")
        print(f"keyword: {args.keyword}")

    if not tqdm_imported:
        print("The library tqdm is not available, so progress bars will not be shown. This will not impact performance.")

    if hasattr(args, 'func'):
        args.func(args)
    else:
        parser.print_help()

if __name__ == "__main__":
    main()


